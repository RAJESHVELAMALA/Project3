{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjhsEmMdj/fe8SsjlTTVX3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAJESHVELAMALA/Project3/blob/main/ImageClassificationV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLD6mhBtk9P0"
      },
      "outputs": [],
      "source": [
        "#Import Os and Basis Libraries\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "#Matplot Images\n",
        "import matplotlib.image as mpimg\n",
        "# Tensflor and Keras Layer and Model and Optimize and Loss\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import *\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "#Kernel Intilizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# import tensorflow_hub as hub\n",
        "from tensorflow.keras.optimizers import Adam , Adamax\n",
        "#PreTrained Model\n",
        "from tensorflow.keras.applications import *\n",
        "#Early Stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# Correct import for ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# Warnings Remove\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# paellet\n",
        "palette = [\"#606060FF\", \"#D6ED17FF\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data and Make DataFrame\n",
        "def L_Data(directory):\n",
        "    filepath =[]\n",
        "    label = []\n",
        "\n",
        "    folds = os.listdir(directory)\n",
        "\n",
        "    for fold in folds:\n",
        "        f_path = os.path.join(directory , fold)\n",
        "\n",
        "        imgs = os.listdir(f_path)\n",
        "\n",
        "        for img in imgs:\n",
        "\n",
        "            img_path = os.path.join(f_path , img)\n",
        "            filepath.append(img_path)\n",
        "            label.append(fold)\n",
        "\n",
        "    #Concat data paths with labels\n",
        "    file_path_series = pd.Series(filepath , name= 'filepath')\n",
        "    Label_path_series = pd.Series(label , name = 'label')\n",
        "    df_train = pd.concat([file_path_series ,Label_path_series ] , axis = 1)\n",
        "\n",
        "    return df_train\n",
        "\n",
        "# # Directory containing the \"Train\" folder\n",
        "directory_T = \"/kaggle/input/fast-food-classification-dataset/Fast Food Classification V2/Train\"\n",
        "# Directory containing the \"Train\" folder\n",
        "directory_TE = \"/kaggle/input/fast-food-classification-dataset/Fast Food Classification V2/Test\"\n",
        "# Train Data and Test Data\n",
        "tr_d = L_Data(directory_T)\n",
        "te_d = L_Data(directory_TE)"
      ],
      "metadata": {
        "id": "lpQyGumpNXCW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "a45de9e5-583d-47c0-ebac-67f2bacf3348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/fast-food-classification-dataset/Fast Food Classification V2/Train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-48add8f7af66>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mdirectory_TE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/fast-food-classification-dataset/Fast Food Classification V2/Test\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Train Data and Test Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mtr_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_Data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mte_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_Data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_TE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-48add8f7af66>\u001b[0m in \u001b[0;36mL_Data\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/fast-food-classification-dataset/Fast Food Classification V2/Train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape\n",
        "print(f\"The shape of The Train data is: {tr_d.shape}\")\n",
        "print(f\"The shape of The Test data is: {te_d.shape}\")"
      ],
      "metadata": {
        "id": "CqSzgruFT8Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data_Dir Train And Test\n",
        "test_dir = directory_TE\n",
        "data_dir = \"/kaggle/working/train_dataset\"\n",
        "\n",
        "# Image Size\n",
        "IMAGE_SIZE = (256, 256)\n",
        "\n",
        "print('Training Images:')\n",
        "# creating the training dataset\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=123,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=32)\n",
        "\n",
        "#Testing Augmented Data\n",
        "print('Validation Images:')\n",
        "validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=123,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=32)\n",
        "\n",
        "# Create an ImageDataGenerator instance without augmentation\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "# Load test data using ImageDataGenerator\n",
        "test_ds = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False ,\n",
        ")"
      ],
      "metadata": {
        "id": "WG5KvJpaT8_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Pixel Values\n",
        "# Train Data\n",
        "train_ds = train_ds.map(lambda x, y: (x / 255.0, y))\n",
        "# Val Data\n",
        "validation_ds = validation_ds.map(lambda x, y: (x / 255.0, y))"
      ],
      "metadata": {
        "id": "1bQeR_28T_l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show COunt Function\n",
        "def Count_S(df, palette):\n",
        "    # Count the occurrences of each category in the 'label' column\n",
        "    count = df['label'].value_counts()\n",
        "\n",
        "    # Create a figure with two subplots\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(20, 6), facecolor='white')\n",
        "\n",
        "    # Plot pie chart on the first subplot\n",
        "    sns.set_palette(palette)\n",
        "    axs[0].pie(count, labels=count.index, autopct='%1.1f%%', startangle=140)\n",
        "    axs[0].set_title('Distribution of Categories')\n",
        "\n",
        "    # Plot bar chart on the second subplot\n",
        "    sns.barplot(x=count.index, y=count.values, ax=axs[1], palette=palette)\n",
        "    axs[1].set_title('Count of Categories')\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "# Define your custom palette\n",
        "custom_palette = palette"
      ],
      "metadata": {
        "id": "MJ8-EWmDUBsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Count\n",
        "Count_S(tr_d, custom_palette)"
      ],
      "metadata": {
        "id": "1od12l9-UD1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Count\n",
        "Count_S(te_d, custom_palette)"
      ],
      "metadata": {
        "id": "8xgajTFQUGvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_images(path, num_images=5):\n",
        "    # Get a list of image filenames in the specified path\n",
        "    image_filenames = os.listdir(path)\n",
        "\n",
        "    # Limit the number of images to visualize if there are more than num_images\n",
        "    num_images = min(num_images, len(image_filenames))\n",
        "\n",
        "    # Create a figure and axis object to display images\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3),facecolor='white')\n",
        "\n",
        "    # Iterate over the selected images and display them\n",
        "    for i, image_filename in enumerate(image_filenames[:num_images]):\n",
        "        # Load the image using Matplotlib\n",
        "        image_path = os.path.join(path, image_filename)\n",
        "        image = mpimg.imread(image_path)\n",
        "\n",
        "        # Display the image\n",
        "        axes[i].imshow(image)\n",
        "        axes[i].axis('off')  # Turn off axis\n",
        "        axes[i].set_title(image_filename)  # Set image filename as title\n",
        "\n",
        "    # Adjust layout and display the figure\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jj8gZQa8UIYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path containing the images to visualize\n",
        "path_to_visualize = \"/kaggle/input/fast-food-classification-dataset/Fast Food Classification V2/Train/Baked Potato\"\n",
        "\n",
        "# Visualize some images from the specified path\n",
        "visualize_images(path_to_visualize, num_images=5)"
      ],
      "metadata": {
        "id": "WxDuW_p8UKbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path containing the images to visualize\n",
        "path_to_visualize = \"/kaggle/input/fast-food-classification-dataset/Fast Food Classification V2/Train/Burger\"\n",
        "\n",
        "# Visualize some images from the specified path\n",
        "visualize_images(path_to_visualize, num_images=5)"
      ],
      "metadata": {
        "id": "jbHrm3qhUMHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path containing the images to visualize\n",
        "path_to_visualize = \"/kaggle/working/train_dataset/Taco\"\n",
        "\n",
        "# Visualize some images from the specified path\n",
        "visualize_images(path_to_visualize, num_images=5)"
      ],
      "metadata": {
        "id": "Y4qgoZA8UOKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path containing the images to visualize\n",
        "path_to_visualize = \"/kaggle/working/train_dataset/Donut\"\n",
        "\n",
        "# Visualize some images from the specified path\n",
        "visualize_images(path_to_visualize, num_images=5)"
      ],
      "metadata": {
        "id": "lW7btAaDUQA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path containing the images to visualize\n",
        "path_to_visualize = \"/kaggle/working/train_dataset/Sandwich\"\n",
        "\n",
        "# Visualize some images from the specified path\n",
        "visualize_images(path_to_visualize, num_images=5)"
      ],
      "metadata": {
        "id": "u3nkKqitUR9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path containing the images to visualize\n",
        "path_to_visualize = \"/kaggle/working/train_dataset/Pizza\"\n",
        "\n",
        "# Visualize some images from the specified path\n",
        "visualize_images(path_to_visualize, num_images=5)"
      ],
      "metadata": {
        "id": "XoQvyzjiUUHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path containing the images to visualize\n",
        "path_to_visualize = \"/kaggle/working/train_dataset/Fries\"\n",
        "\n",
        "# Visualize some images from the specified path\n",
        "visualize_images(path_to_visualize, num_images=5)"
      ],
      "metadata": {
        "id": "fKIhnTA3UVsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the URL of the model\n",
        "url = \"https://tfhub.dev/google/bit/m-r50x1/1\"\n",
        "\n",
        "# Load the model from the URL\n",
        "model_E = hub.KerasLayer(url)\n",
        "\n",
        "# Set the model to be non-trainable\n",
        "model_E.trainable = False"
      ],
      "metadata": {
        "id": "XoeXJNmjUXws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def M_B(model_E ,EPO):\n",
        "    # Define the name for your model\n",
        "    model_name = \"Abdullah_Food_Classification_Model\"\n",
        "    # Build the model\n",
        "    model = Sequential(name=model_name)\n",
        "\n",
        "    # Add the pre-trained DenseNet121_base\n",
        "    model.add(model_E)\n",
        "\n",
        "    # Batch Normalization\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Dropout\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # # Add a dense layer with 220 units and ReLU activation function\n",
        "    # model.add(Dense(120, activation='relu'))\n",
        "\n",
        "    # Add the output layer with 10 units and Softmax activation function\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Build the model\n",
        "    model.build((None, 256, 256, 3))\n",
        "\n",
        "    # Print model summary\n",
        "    print(model.summary())\n",
        "\n",
        "    #Early_Stopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "    #Fitting Model\n",
        "    history = model.fit_generator(train_ds,\n",
        "                            epochs= EPO,\n",
        "                            validation_data = validation_ds,\n",
        "                            callbacks = early_stopping)\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "6JQ1DB4eUZz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = M_B(model_E, 6)"
      ],
      "metadata": {
        "id": "JD4Fn5ASUcLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the validation dataset\n",
        "validation_loss, validation_accuracy = model.evaluate(validation_ds)\n",
        "\n",
        "# Print the validation loss and accuracy\n",
        "print(\"Validation Loss:\", validation_loss)\n",
        "print(\"Validation Accuracy:\", validation_accuracy)"
      ],
      "metadata": {
        "id": "ptn-fp2OUeTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the epoch with the highest validation accuracy\n",
        "best_epoch = history.history['val_accuracy'].index(max(history.history['val_accuracy'])) + 1\n",
        "\n",
        "# Set the background style\n",
        "plt.style.use('seaborn-darkgrid')\n",
        "\n",
        "# Create a subplot with 1 row and 2 columns\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "axs[0].plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
        "axs[0].plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
        "axs[0].scatter(best_epoch - 1, history.history['val_accuracy'][best_epoch - 1], color='green', label=f'Best Epoch: {best_epoch}')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Accuracy')\n",
        "axs[0].set_title('Training and Validation Accuracy')\n",
        "axs[0].legend()\n",
        "\n",
        "# Plot training and validation loss\n",
        "axs[1].plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "axs[1].plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "axs[1].scatter(best_epoch - 1, history.history['val_loss'][best_epoch - 1], color='green',label=f'Best Epoch: {best_epoch}')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Loss')\n",
        "axs[1].set_title('Training and Validation Loss')\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xwjoW2SOUgVZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}